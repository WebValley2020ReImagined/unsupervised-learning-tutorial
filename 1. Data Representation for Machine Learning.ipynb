{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is about creating models from data: for that reason, we'll start by discussing how data can be represented in order to be processed correctly by **Machine Learning** (i.e. `Data` $\\mapsto$ `Machine Learning`).\n",
    "\n",
    "Indeed, it would be worthwhile investigating this relationship in the other direction, i.e. `Machine learnig` $\\mapsto$ `Data`$^{[1]}$, but this is beyond of the scope of this tutorial. We will briefly touch base on this in the next notebook.\n",
    "\n",
    "<span class=\"fn\"><i>[1]</i>Although nowadays the answer to this question always seems to be **Deep Learning**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Machine Learning\n",
    "\n",
    "Despite what you might probably expect, Machine learning algorithm are pretty _strict_ about what input data should look like - and so it is `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in **scikit-learn**, with very few exceptions, is assumed to be stored as a\n",
    "**two-dimensional array**, of size `[n_samples, n_features]`. \n",
    "\n",
    "This array is usually referrred as the **feature matrix**.\n",
    "\n",
    "There is also the **label vector**$^{[2]}$, of size `n_samples`, containing the list of labels for each sample.\n",
    "\n",
    "$$\n",
    "{\\rm feature~matrix:~~~} {\\bf X}~=~\\left[\n",
    "\\begin{matrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1D}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2D}\\\\\n",
    "x_{31} & x_{32} & \\cdots & x_{3D}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{N1} & x_{N2} & \\cdots & x_{ND}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm label~vector:~~~} {\\bf y}~=~ [y_1, y_2, y_3, \\cdots y_N]\n",
    "$$\n",
    "\n",
    "Here, $N$ is `n_samples`; $D$ is `n_features`.\n",
    "\n",
    "Each sample (data point) is a row in the data array, and each feature is a column.\n",
    "\n",
    "<span class=\"fn\"><i>[2]</i> The **label vector** only applies to **Supervised** learning settings. More on this later.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $N$ (`n_samples`):   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- $D$ (`n_features`):  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. \n",
    "\n",
    "However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. \n",
    "\n",
    "This is a case where **sparse** matrices can be useful, in that they are\n",
    "much more memory-efficient. In this cases, `scipy.sparse` offers a more efficienti solution than **dense** `numpy` arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few Notes on `Sparse` and Data Sparsity\n",
    "\n",
    "**Scipy** provides `sparse` matrix data structures which are optimized for storing sparse data. \n",
    "\n",
    "The main feature of sparse formats is that you don’t store zeros so if your data is \n",
    "sparse then you use much less memory. \n",
    "\n",
    "A non-zero value in a sparse (`CSR` or `CSC` [Scipy Doc](https://docs.scipy.org/doc/scipy/reference/sparse.html)) representation will only take on \n",
    "average one `32bit` integer position + the `64bit` floating point value + an \n",
    "additional `32bit` per row or column in the matrix. \n",
    "\n",
    "Using sparse input on a dense (or sparse) linear model can speedup prediction by \n",
    "quite a bit as only the non zero valued features impact the dot product and thus \n",
    "the model predictions. Hence if you have `100` non-zeros in `1e6` dimensional \n",
    "space, you only need `100` multiply and add operation instead of `1e6`.\n",
    "\n",
    "Calculation over a dense representation, however, may leverage highly optimised \n",
    "vector operations and multithreading in **BLAS**, and tends to result in fewer CPU \n",
    "cache misses. So the sparsity should typically be quite high (`10%` non-zeros max, \n",
    "to be checked depending on the hardware) for the sparse input representation to be \n",
    "faster than the dense input representation on a machine with many CPUs and an \n",
    "optimized BLAS implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a test function to check the sparsity of your data:\n",
    "\n",
    "```python\n",
    "def sparsity_ratio(X):\n",
    "    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\n",
    "print(\"input sparsity ratio:\", sparsity_ratio(X))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb you can consider that if the sparsity ratio is greater than `90%` you can probably benefit from sparse formats. \n",
    "\n",
    "Check Scipy’s sparse matrix formats documentation for more information on how to build (or convert your data to) sparse matrix formats. \n",
    "\n",
    "$\\rightarrow$ Most of the time the `CSR` and `CSC` formats work best.\n",
    "\n",
    "<span class=\"fn\"><i>Adapted from:</i> [Scikit-learn Doc :: Computing](https://scikit-learn.org/stable/modules/computing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset API in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn import datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **three** main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.\n",
    "\n",
    "1. The dataset **loaders**:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_\n",
    "```\n",
    "\n",
    "They can be used to load small standard datasets (also referred to as *Toy datasets*).\n",
    "\n",
    "\n",
    "2. The dataset **fetchers**:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_\n",
    "```\n",
    "\n",
    "They can be used to download and load larger datasets (also referred to as *Real world datasets*).\n",
    "\n",
    "Both loaders and fetchers functions return a `sklearn.utils.Bunch` object holding \n",
    "at least two items: an array of shape `n_samples * n_features` with key data \n",
    "(except for 20newsgroups) and a `numpy` array of length `n_samples`, containing the \n",
    "target values, with key target.\n",
    "\n",
    "The `Bunch` object is a dictionary that exposes its keys are attributes.\n",
    "\n",
    "It’s also possible for almost all of these function to constrain the output to be a \n",
    "tuple containing only the data and the target, by setting the `return_X_y` \n",
    "parameter to `True`.\n",
    "\n",
    "The datasets also contain a full description in their `DESCR` attribute and some \n",
    "contain `feature_names` and `target_names`. \n",
    "\n",
    "The dataset **generation** functions:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_\n",
    "```\n",
    "\n",
    "They can be used to generate controlled **synthetic datasets**.\n",
    "\n",
    "These functions return a `tuple` `(X, y)` consisting of a `n_samples * n_features` \n",
    "`numpy` array `X` and an array of length `n_samples` containing the targets `y`.\n",
    "\n",
    "In addition, there are also miscellaneous tools to load datasets of other formats or from other locations (e.g. `fetch_openml`).\n",
    "\n",
    "---\n",
    "\n",
    "<span class=\"fn\"><i>Adapted from</i> [Scikit-learn Documentation :: Datasets](https://scikit-learn.org/stable/datasets/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aquaintance with the API\n",
    "\n",
    "Leverage on the auto-completion feature of Jupyter notebooks or your code editor (e.g. Visual Studio Code) to see what are supported datasets and functions included in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.datasets import load_<TAB>\n",
    "from sklearn.datasets import fetch_<TAB>\n",
    "from sklearn.datasets import make_<TAB>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Iris data (again) under the \"data representation\" light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = X.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length (cm)    5.1\n",
       "sepal width (cm)     3.5\n",
       "petal length (cm)    1.4\n",
       "petal width (cm)     0.2\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A new (?) Dataset\n",
    "\n",
    "Now we'll take a look at another dataset, one where we have to put a bit more thought into how to represent the data: the `digits` data (a.k.a. **MNIST**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore the dataset keys and stored information\n",
    "\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra (if we have time)\n",
    "\n",
    "Try to `fetch` and `load` the RCV1-v2 dataset and check the `type` of `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv1 = fetch_rcv1()  # This may take some time depending on your internet connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'sample_id', 'target_names', 'DESCR'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _rcv1_dataset:\n",
      "\n",
      "RCV1 dataset\n",
      "------------\n",
      "\n",
      "Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually \n",
      "categorized newswire stories made available by Reuters, Ltd. for research \n",
      "purposes. The dataset is extensively described in [1]_.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    ==============     =====================\n",
      "    Classes                              103\n",
      "    Samples total                     804414\n",
      "    Dimensionality                     47236\n",
      "    Features           real, between 0 and 1\n",
      "    ==============     =====================\n",
      "\n",
      ":func:`sklearn.datasets.fetch_rcv1` will load the following \n",
      "version: RCV1-v2, vectors, full sets, topics multilabels::\n",
      "\n",
      "    >>> from sklearn.datasets import fetch_rcv1\n",
      "    >>> rcv1 = fetch_rcv1()\n",
      "\n",
      "It returns a dictionary-like object, with the following attributes:\n",
      "\n",
      "``data``:\n",
      "The feature matrix is a scipy CSR sparse matrix, with 804414 samples and\n",
      "47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors.\n",
      "A nearly chronological split is proposed in [1]_: The first 23149 samples are\n",
      "the training set. The last 781265 samples are the testing set. This follows \n",
      "the official LYRL2004 chronological split. The array has 0.16% of non zero \n",
      "values::\n",
      "\n",
      "    >>> rcv1.data.shape\n",
      "    (804414, 47236)\n",
      "\n",
      "``target``:\n",
      "The target values are stored in a scipy CSR sparse matrix, with 804414 samples \n",
      "and 103 categories. Each sample has a value of 1 in its categories, and 0 in \n",
      "others. The array has 3.15% of non zero values::\n",
      "\n",
      "    >>> rcv1.target.shape\n",
      "    (804414, 103)\n",
      "\n",
      "``sample_id``:\n",
      "Each sample can be identified by its ID, ranging (with gaps) from 2286 \n",
      "to 810596::\n",
      "\n",
      "    >>> rcv1.sample_id[:3]\n",
      "    array([2286, 2287, 2288], dtype=uint32)\n",
      "\n",
      "``target_names``:\n",
      "The target values are the topics of each sample. Each sample belongs to at \n",
      "least one topic, and to up to 17 topics. There are 103 topics, each \n",
      "represented by a string. Their corpus frequencies span five orders of \n",
      "magnitude, from 5 occurrences for 'GMIL', to 381327 for 'CCAT'::\n",
      "\n",
      "    >>> rcv1.target_names[:3].tolist()  # doctest: +SKIP\n",
      "    ['E11', 'ECAT', 'M11']\n",
      "\n",
      "The dataset will be downloaded from the `rcv1 homepage`_ if necessary.\n",
      "The compressed size is about 656 MB.\n",
      "\n",
      ".. _rcv1 homepage: http://jmlr.csail.mit.edu/papers/volume5/lewis04a/\n",
      "\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    .. [1] Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). \n",
      "           RCV1: A new benchmark collection for text categorization research. \n",
      "           The Journal of Machine Learning Research, 5, 361-397.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rcv1.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rcv1.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(804414, 47236)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this!\n",
    "type(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL-TORCH)",
   "language": "python",
   "name": "dl-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
